{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebd062e8",
   "metadata": {},
   "source": [
    "# üé® Data Designer Tutorial: Providing Images as Context for Vision-Based Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce5b7f7",
   "metadata": {},
   "source": [
    "#### üìö What you'll learn\n",
    "\n",
    "This notebook demonstrates how to provide images as context to generate text descriptions using vision-language models.\n",
    "\n",
    "- ‚ú® **Visual Document Processing**: Converting images to chat-ready format for model consumption\n",
    "- üîç **Vision-Language Generation**: Using vision models to generate detailed summaries from images\n",
    "\n",
    "If this is your first time using Data Designer, we recommend starting with the [first notebook](/notebooks/1-the-basics/) in this tutorial series.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d0bfd4",
   "metadata": {},
   "source": [
    "### ‚ö° Colab Setup\n",
    "\n",
    "Run the cells below to install the dependencies and set up the API key. If you don't have an API key, you can generate one from [build.nvidia.com](https://build.nvidia.com).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661740a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU data-designer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c4188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pillow>=12.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaeb938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "from google.colab import userdata\n",
    "\n",
    "try:\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = userdata.get(\"NVIDIA_API_KEY\")\n",
    "except userdata.SecretNotFoundError:\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aac5e8",
   "metadata": {},
   "source": [
    "### üì¶ Import the essentials\n",
    "\n",
    "- The `essentials` module provides quick access to the most commonly used objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b905ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import base64\n",
    "import io\n",
    "import uuid\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import rich\n",
    "from datasets import load_dataset\n",
    "from IPython.display import display\n",
    "from rich.panel import Panel\n",
    "\n",
    "# Data Designer imports\n",
    "from data_designer.essentials import (\n",
    "    DataDesigner,\n",
    "    DataDesignerConfigBuilder,\n",
    "    ImageContext,\n",
    "    ImageFormat,\n",
    "    InferenceParameters,\n",
    "    LLMTextColumnConfig,\n",
    "    ModalityDataType,\n",
    "    ModelConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f508d655",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Initialize the Data Designer interface\n",
    "\n",
    "- `DataDesigner` is the main object is responsible for managing the data generation process.\n",
    "\n",
    "- When initialized without arguments, the [default model providers](https://nvidia-nemo.github.io/DataDesigner/concepts/models/default-model-settings/) are used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0607008",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_designer = DataDesigner()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0467d8c",
   "metadata": {},
   "source": [
    "### üéõÔ∏è Define model configurations\n",
    "\n",
    "- Each `ModelConfig` defines a model that can be used during the generation process.\n",
    "\n",
    "- The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).\n",
    "\n",
    "- The \"model provider\" is the external service that hosts the model (see the [model config](https://nvidia-nemo.github.io/DataDesigner/concepts/models/default-model-settings/) docs for more details).\n",
    "\n",
    "- By default, we use [build.nvidia.com](https://build.nvidia.com/models) as the model provider.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4d9a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This name is set in the model provider configuration.\n",
    "MODEL_PROVIDER = \"nvidia\"\n",
    "\n",
    "model_configs = [\n",
    "    ModelConfig(\n",
    "        alias=\"vision\",\n",
    "        model=\"meta/llama-4-scout-17b-16e-instruct\",\n",
    "        provider=MODEL_PROVIDER,\n",
    "        inference_parameters=InferenceParameters(\n",
    "            temperature=0.60,\n",
    "            top_p=0.95,\n",
    "            max_tokens=2048,\n",
    "        ),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d66a8a6",
   "metadata": {},
   "source": [
    "### üèóÔ∏è Initialize the Data Designer Config Builder\n",
    "\n",
    "- The Data Designer config defines the dataset schema and generation process.\n",
    "\n",
    "- The config builder provides an intuitive interface for building this configuration.\n",
    "\n",
    "- The list of model configs is provided to the builder at initialization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad9b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder = DataDesignerConfigBuilder(model_configs=model_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75935c33",
   "metadata": {},
   "source": [
    "### üå± Seed Dataset Creation\n",
    "\n",
    "In this section, we'll prepare our visual documents as a seed dataset for summarization:\n",
    "\n",
    "- **Loading Visual Documents**: We use the ColPali dataset containing document images\n",
    "- **Image Processing**: Convert images to base64 format for vision model consumption\n",
    "- **Metadata Extraction**: Preserve relevant document information (filename, page number, source, etc.)\n",
    "\n",
    "The seed dataset will be used to generate detailed text summaries of each document image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868e41af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset processing configuration\n",
    "IMG_COUNT = 512  # Number of images to process\n",
    "BASE64_IMAGE_HEIGHT = 512  # Standardized height for model input\n",
    "\n",
    "# Load ColPali dataset for visual documents\n",
    "img_dataset_cfg = {\"path\": \"vidore/colpali_train_set\", \"split\": \"train\", \"streaming\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5466e56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, height: int):\n",
    "    \"\"\"\n",
    "    Resize image while maintaining aspect ratio.\n",
    "\n",
    "    Args:\n",
    "        image: PIL Image object\n",
    "        height: Target height in pixels\n",
    "\n",
    "    Returns:\n",
    "        Resized PIL Image object\n",
    "    \"\"\"\n",
    "    original_width, original_height = image.size\n",
    "    width = int(original_width * (height / original_height))\n",
    "    return image.resize((width, height))\n",
    "\n",
    "\n",
    "def convert_image_to_chat_format(record, height: int) -> dict:\n",
    "    \"\"\"\n",
    "    Convert PIL image to base64 format for chat template usage.\n",
    "\n",
    "    Args:\n",
    "        record: Dataset record containing image and metadata\n",
    "        height: Target height for image resizing\n",
    "\n",
    "    Returns:\n",
    "        Updated record with base64_image and uuid fields\n",
    "    \"\"\"\n",
    "    # Resize image for consistent processing\n",
    "    image = resize_image(record[\"image\"], height)\n",
    "\n",
    "    # Convert to base64 string\n",
    "    img_buffer = io.BytesIO()\n",
    "    image.save(img_buffer, format=\"PNG\")\n",
    "    byte_data = img_buffer.getvalue()\n",
    "    base64_encoded_data = base64.b64encode(byte_data)\n",
    "    base64_string = base64_encoded_data.decode(\"utf-8\")\n",
    "\n",
    "    # Return updated record\n",
    "    return record | {\"base64_image\": base64_string, \"uuid\": str(uuid.uuid4())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbca6568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process the visual document dataset\n",
    "print(\"üì• Loading and processing document images...\")\n",
    "\n",
    "img_dataset_iter = iter(\n",
    "    load_dataset(**img_dataset_cfg).map(convert_image_to_chat_format, fn_kwargs={\"height\": BASE64_IMAGE_HEIGHT})\n",
    ")\n",
    "img_dataset = pd.DataFrame([next(img_dataset_iter) for _ in range(IMG_COUNT)])\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(img_dataset)} images with columns: {list(img_dataset.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce17e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0faf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the seed dataset containing our processed images\n",
    "df_seed = pd.DataFrame(img_dataset)[[\"uuid\", \"image_filename\", \"base64_image\", \"page\", \"options\", \"source\"]]\n",
    "config_builder.with_seed_dataset(\n",
    "    DataDesigner.make_seed_reference_from_dataframe(df_seed, file_path=\"colpali_train_set.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bc8b77",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Add a column to generate detailed document summaries\n",
    "config_builder.add_column(\n",
    "    LLMTextColumnConfig(\n",
    "        name=\"summary\",\n",
    "        model_alias=\"vision\",\n",
    "        prompt=(\n",
    "            \"Provide a detailed summary of the content in this image in Markdown format. \"\n",
    "            \"Start from the top of the image and then describe it from top to bottom. \"\n",
    "            \"Place a summary at the bottom.\"\n",
    "        ),\n",
    "        multi_modal_context=[\n",
    "            ImageContext(\n",
    "                column_name=\"base64_image\",\n",
    "                data_type=ModalityDataType.BASE64,\n",
    "                image_format=ImageFormat.PNG,\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ac3f58",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d13a0e4",
   "metadata": {},
   "source": [
    "### üîÅ Iteration is key ‚Äì preview the dataset!\n",
    "\n",
    "1. Use the `preview` method to generate a sample of records quickly.\n",
    "\n",
    "2. Inspect the results for quality and format issues.\n",
    "\n",
    "3. Adjust column configurations, prompts, or parameters as needed.\n",
    "\n",
    "4. Re-run the preview until satisfied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336af89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preview = data_designer.preview(config_builder, num_records=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a57a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell multiple times to cycle through the 2 preview records.\n",
    "preview.display_sample_record()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e05b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The preview dataset is available as a pandas DataFrame.\n",
    "preview.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69d543e",
   "metadata": {},
   "source": [
    "### üìä Analyze the generated data\n",
    "\n",
    "- Data Designer automatically generates a basic statistical analysis of the generated data.\n",
    "\n",
    "- This analysis is available via the `analysis` property of generation result objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cb66a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the analysis as a table.\n",
    "preview.analysis.to_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60815241",
   "metadata": {},
   "source": [
    "### üîé Visual Inspection\n",
    "\n",
    "Let's compare the original document image with the generated summary to validate quality:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9dddf6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Compare original document with generated summary\n",
    "index = 0  # Change this to view different examples\n",
    "\n",
    "# Merge preview data with original images for comparison\n",
    "comparison_dataset = preview.dataset.merge(pd.DataFrame(img_dataset)[[\"uuid\", \"image\"]], how=\"left\", on=\"uuid\")\n",
    "\n",
    "# Extract the record for display\n",
    "record = comparison_dataset.iloc[index]\n",
    "\n",
    "print(\"üìÑ Original Document Image:\")\n",
    "display(resize_image(record.image, BASE64_IMAGE_HEIGHT))\n",
    "\n",
    "print(\"\\nüìù Generated Summary:\")\n",
    "rich.print(Panel(record.summary, title=\"Document Summary\", title_align=\"left\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6e2469",
   "metadata": {},
   "source": [
    "### üÜô Scale up!\n",
    "\n",
    "- Happy with your preview data?\n",
    "\n",
    "- Use the `create` method to submit larger Data Designer generation jobs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a0ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = data_designer.create(config_builder, num_records=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255b8f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the generated dataset as a pandas DataFrame.\n",
    "dataset = results.load_dataset()\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b935b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the analysis results into memory.\n",
    "analysis = results.load_analysis()\n",
    "\n",
    "analysis.to_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74f1121",
   "metadata": {},
   "source": [
    "## ‚è≠Ô∏è Next Steps\n",
    "\n",
    "Now that you've learned how to use visual context for image summarization in Data Designer, explore more:\n",
    "\n",
    "- Experiment with different vision models for specific document types\n",
    "- Try different prompt variations to generate specialized descriptions (e.g., technical details, key findings)\n",
    "- Combine vision-based summaries with other column types for multi-modal workflows\n",
    "- Apply this pattern to other vision tasks like image captioning, OCR validation, or visual question answering\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
